{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "C:\\Users\\shawn\\AppData\\Local\\Temp\\ipykernel_8980\\3492377944.py:19: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\")\n",
      "c:\\Users\\shawn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs=[WebBaseLoader(url).load() for url in urls]\n",
    "doc_list=[item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=50, chunk_overlap=0)\n",
    "\n",
    "doc_splits=text_splitter.split_documents(doc_list)\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "vectorstore=Chroma.from_documents(doc_splits, embeddings,collection_name=\"rag-chroma\")\n",
    "\n",
    "retriever=vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel,Field\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no' \"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(\n",
    "    temperature=0,\n",
    "    groq_api_key=\"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\",\n",
    "    model=\"llama-3.1-70b-versatile\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shawn\\AppData\\Local\\Temp\\ipykernel_8980\\3332864624.py:14: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs=retriever.get_relevant_documents(question)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "structured_llm_grader=llm.with_structured_output(GradeDocuments)\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader=grade_prompt | structured_llm_grader\n",
    "question=\"agent_memory\"\n",
    "docs=retriever.get_relevant_documents(question)\n",
    "doc_text=docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"document\": doc_text, \"question\": question}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shawn\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langsmith\\client.py:323: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a LLM-powered autonomous agent system, there are two types of memory: short-term memory and long-term memory. Short-term memory utilizes in-context learning, while long-term memory provides the capability to retain and recall information over extended periods, often by leveraging an external vector store and fast retrieval. The agent's memory module is designed to record a comprehensive list of agents' experience in natural language.\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_dosc(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain=prompt | llm | StrOutputParser()\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is agent memory in artificial intelligence and how does it work?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n \n",
    "     for web search. Look at the input and try to reason about the underlying semantic intent / meaning.\n",
    "     Just output the rewritten question, do not add any other text. \\n\\n\"\"\"\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "question_rewriter=re_write_prompt | llm | StrOutputParser()\n",
    "question_rewriter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "web_search_tool=DuckDuckGoSearchResults(max_results=3)\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "wikipedia_tool= WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "def retrieve(state):\n",
    "    print('RETRIEVE')\n",
    "    question=state[\"question\"]\n",
    "    documents=retriever.get_relevant_documents(question)\n",
    "    return {\"documents\": documents,\"question\": question}\n",
    "\n",
    "def generate(state):\n",
    "    print('GENERATE')\n",
    "    question=state[\"question\"]\n",
    "    documents=state[\"documents\"]\n",
    "    generation=rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "def grade_documents(state):\n",
    "\n",
    "    print('CHECK RELEVANCE')\n",
    "    question=state[\"question\"]\n",
    "    documents=state[\"documents\"]\n",
    "    \n",
    "    filtered_docs=[]\n",
    "    web_search=\"no\"\n",
    "    for d in documents:\n",
    "        score=retrieval_grader.invoke({\"document\": d.page_content, \"question\": question})\n",
    "        grade=score.binary_score\n",
    "        if grade==\"yes\":\n",
    "            print('DOCUMENT IS RELEVANT')\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print('DOCUMENT IS NOT RELEVANT')\n",
    "            web_search=\"yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \"web_search\": web_search}\n",
    "\n",
    "\n",
    "\n",
    "def transform_query(state):\n",
    "    print('TRANSFORM QUERY')\n",
    "    question=state[\"question\"]\n",
    "    documents=state[\"documents\"]\n",
    "    new_question=question_rewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": state[\"documents\"], \"question\": new_question}\n",
    "\n",
    "\n",
    "def web_search(state):\n",
    "    print('WEB SEARCH')\n",
    "    question=state[\"question\"]\n",
    "    documents=state[\"documents\"]\n",
    "    search_results=wikipedia_tool.run(question)\n",
    "    web_results=Document(page_content=search_results)\n",
    "    documents.append(web_results)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    state[\"documents\"]\n",
    "\n",
    "    if web_search == \"yes\":\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
    "        )\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START,END,StateGraph\n",
    "\n",
    "workflow=StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"retriever\",retrieve)\n",
    "workflow.add_node(\"grader\",grade_documents)\n",
    "workflow.add_node(\"generate\",generate)\n",
    "workflow.add_node(\"transform_query\",transform_query)\n",
    "workflow.add_node(\"web_search_node\",web_search)\n",
    "\n",
    "workflow.add_edge(START,\"retriever\")\n",
    "workflow.add_edge(\"retriever\", \"grader\")\n",
    "workflow.add_conditional_edges(\"grader\",decide_to_generate,{\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    })\n",
    "workflow.add_edge(\"transform_query\", \"web_search_node\")\n",
    "workflow.add_edge(\"web_search_node\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "app=workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RETRIEVE\n",
      "\"Node 'retriever':\"\n",
      "'\\n---\\n'\n",
      "CHECK RELEVANCE\n",
      "DOCUMENT IS RELEVANT\n",
      "DOCUMENT IS RELEVANT\n",
      "DOCUMENT IS RELEVANT\n",
      "DOCUMENT IS RELEVANT\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'grader':\"\n",
      "'\\n---\\n'\n",
      "GENERATE\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "('AI, or Artificial Intelligence, refers to a powerful general problem solver '\n",
      " 'that can be framed as a brain in an autonomous agent system, utilizing '\n",
      " 'components like planning, memory, and tool use to handle complex tasks. It '\n",
      " 'can generate well-written copies, stories, essays, and programs, and can '\n",
      " 'also learn from mistakes and refine its actions. The provided context does '\n",
      " 'not give a comprehensive definition of AI, but it highlights its potential '\n",
      " 'and applications in LLM-powered autonomous agent systems.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Run\n",
    "inputs = {\"question\": \"What is AI\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint(f\"Node '{key}':\")\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
